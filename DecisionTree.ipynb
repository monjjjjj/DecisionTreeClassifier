{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjGBE5bjzCCsdNWmnzMFNH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monjjjjj/DecisionTreeClassifier/blob/main/DecisionTree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVHscqAv8U47"
      },
      "outputs": [],
      "source": [
        "import sklearn.metrics\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class Node:\n",
        "    \"Decision tree node\"\n",
        "    def __init__(self, entropy, num_samples, num_samples_per_class, predicted_class, num_errors, alpha=float(\"inf\")):\n",
        "        self.entropy = entropy # the entropy of current node\n",
        "        self.num_samples = num_samples\n",
        "        self.num_samples_per_class = num_samples_per_class\n",
        "        self.predicted_class = predicted_class # the majority class of the split group\n",
        "        self.feature_index = 0 # the feature index we used to split the node\n",
        "        self.threshold = 0 # for binary split\n",
        "        self.left = None # left child node\n",
        "        self.right = None # right child node\n",
        "        self.num_errors = num_errors # error after cut\n",
        "        self.alpha = alpha # each node alpha\n",
        "\n",
        "\n",
        "class DecisionTreeClassifier:\n",
        "    def __init__(self, max_depth=4):\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def _entropy(self, sample_y, n_classes):\n",
        "        # TODO: calculate the entropy of sample_y and return it\n",
        "        # sample_y represent the label of node\n",
        "        # entropy = -sum(pi *log2(pi))\n",
        "        entropy = 0\n",
        "        probs = []\n",
        "        total = len(sample_y)\n",
        "\n",
        "        for n in range(n_classes):\n",
        "            if len(sample_y[sample_y == n]) != 0 and len(sample_y) != 0:\n",
        "                probs = np.append(probs, np.sum(sample_y == n))\n",
        "            else:\n",
        "                entropy = 0\n",
        "\n",
        "        probs = probs / total\n",
        "\n",
        "        for prob in probs:\n",
        "            entropy = np.sum(-prob*math.log(prob, 2))\n",
        "\n",
        "        return entropy\n",
        "\n",
        "    def _feature_split(self, X, y, n_classes):\n",
        "        # Returns:\n",
        "        #  best_idx: Index of the feature for best split, or None if no split is found.\n",
        "        #  best_thr: Threshold to use for the split, or None if no split is found.\n",
        "        DEFAULT = None, None\n",
        "\n",
        "        m = y.size\n",
        "\n",
        "        if m <= 1:\n",
        "            return DEFAULT\n",
        "        # Entropy of current node.\n",
        "        best_criterion = self._entropy(y, n_classes)\n",
        "\n",
        "        best_idx, best_thr = DEFAULT\n",
        "        # TODO: find the best split, loop through all the features, and consider all the\n",
        "        # midpoints between adjacent training samples as possible thresholds. \n",
        "        # Compute the Entropy impurity of the split generated by that particular feature/threshold\n",
        "        # pair, and return the pair with smallest impurity.\n",
        "        best = 0\n",
        "        for i in range(len(X[0])):\n",
        "            data = X[:, i]\n",
        "            thr_list = []\n",
        "\n",
        "            for j in range(len(set(data)) - 1):\n",
        "                thr_list.append((sorted(set(data))[j] + sorted(set(data))[j + 1]) / 2)\n",
        "\n",
        "            for thr in thr_list:\n",
        "                rate = len(y[data <= thr]) / len(y)\n",
        "\n",
        "                info = rate * self._entropy(y[data <= thr], n_classes) + (1 - rate) * self._entropy(y[data > thr],\n",
        "                                                                                                    n_classes)\n",
        "                if best < best_criterion - info:\n",
        "                    best = best_criterion - info\n",
        "                    best_idx = i\n",
        "                    best_thr = thr\n",
        "\n",
        "        return best_idx, best_thr\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]\n",
        "        predicted_class = np.argmax(num_samples_per_class)\n",
        "        correct_label_num = num_samples_per_class[predicted_class]\n",
        "        num_errors = y.size - correct_label_num\n",
        "        node = Node(\n",
        "            entropy=self._entropy(y, self.n_classes_),\n",
        "            num_samples=y.size,\n",
        "            num_samples_per_class=num_samples_per_class,\n",
        "            predicted_class=predicted_class,\n",
        "            num_errors=num_errors\n",
        "        )\n",
        "\n",
        "        if depth < self.max_depth:\n",
        "            idx, thr = self._feature_split(X, y, self.n_classes_)\n",
        "            node.feature_index = idx\n",
        "            node.threshold = thr\n",
        "            if idx is not None:\n",
        "            # TODO: Split the tree recursively according index and threshold until maximum depth is reached.\n",
        "                node.left = self._build_tree(X[X[:, idx] <= thr], y[X[:, idx] <= thr], depth + 1)\n",
        "                node.right = self._build_tree(X[X[:, idx] > thr], y[X[:, idx] > thr], depth + 1)\n",
        "                pass\n",
        "        return node\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        # TODO\n",
        "        # Fits to the given training data\n",
        "        self.n_classes_ = len(set(Y))\n",
        "        self.tree_ = self._build_tree(X, Y)\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        pred = []\n",
        "        #TODO: predict the label of data\n",
        "        for i in X[:]:\n",
        "            node = self.tree_\n",
        "            while node.left is not None and node.right is not None:\n",
        "                if i[node.feature_index] <= node.threshold:\n",
        "                    node = node.left\n",
        "                else:\n",
        "                    node = node.right\n",
        "            pred.append(node.predicted_class)\n",
        "        return pred\n",
        "\n",
        "    def _find_leaves(self, root):\n",
        "        #TODO\n",
        "        ## find each node child leaves number\n",
        "        node_list = [root]\n",
        "        leaf = 0\n",
        "\n",
        "        while len(node_list) != 0:\n",
        "            node = node_list[-1]\n",
        "            node_list.pop()\n",
        "            if node.left == None and node.right == None:\n",
        "                leaf = leaf + 1\n",
        "            else:\n",
        "                node_list.append(node.right)\n",
        "                node_list.append(node.left)\n",
        "        return leaf\n",
        "\n",
        "    def _error_before_cut(self, root):\n",
        "        # TODO\n",
        "        ## return error before post-pruning\n",
        "        node_list = [root]\n",
        "        error = 0\n",
        "\n",
        "        while len(node_list) != 0:\n",
        "            node = node_list[-1]\n",
        "            node_list.pop()\n",
        "            if node.left == None and node.right == None:\n",
        "                error = error + node.num_errors\n",
        "            else:\n",
        "                node_list.append(node.right)\n",
        "                node_list.append(node.left)\n",
        "        return error\n",
        "\n",
        "    def _compute_alpha(self, root):\n",
        "        # TODO\n",
        "        ## Compute each node alpha\n",
        "        # alpha = (error after cut - error before cut) / (leaves been cut - 1)\n",
        "        node_list = [root]\n",
        "        while len(node_list) != 0:\n",
        "            node = node_list[-1]\n",
        "            node_list.pop()\n",
        "            leaf = self._find_leaves(node)\n",
        "            if leaf > 1:\n",
        "                node.alpha = (node.num_errors - self._error_before_cut(node)) / (leaf - 1)\n",
        "\n",
        "            if node.left is not None and node.right is not None:\n",
        "                node_list.append(node.right)\n",
        "                node_list.append(node.left)\n",
        "        pass\n",
        "\n",
        "    def _find_min_alpha(self, root):\n",
        "        MinAlpha = float(\"inf\")\n",
        "        # TODO\n",
        "        ## Search the Decision tree which have minimum alpha's node\n",
        "        node_list = [root]\n",
        "        while len(node_list) != 0:\n",
        "            node = node_list[-1]\n",
        "            node_list.pop()\n",
        "\n",
        "            if self._find_leaves(node) > 1:\n",
        "                if MinAlpha > node.alpha:\n",
        "                    MinAlpha = node.alpha\n",
        "\n",
        "            if node.left is not None and node.right is not None:\n",
        "                node_list.append(node.right)\n",
        "                node_list.append(node.left)\n",
        "        return MinAlpha\n",
        "\n",
        "    def _prune(self):\n",
        "        # TODO\n",
        "        # prune the decision tree with minimum alpha node\n",
        "        node_list = [self.tree_]\n",
        "        self._compute_alpha(self.tree_)\n",
        "        cut_node = self._find_min_alpha(self.tree_)\n",
        "\n",
        "        while len(node_list) != 0:\n",
        "            node = node_list[-1]\n",
        "            node_list.pop()\n",
        "\n",
        "            if self._find_leaves(node) > 1:\n",
        "                if cut_node == node.alpha:\n",
        "                    node.left = None\n",
        "                    node.right = None\n",
        "                    return\n",
        "            if node.left is not None and node.right is not None:\n",
        "                node_list.append(node.right)\n",
        "                node_list.append(node.left)\n",
        "        pass\n",
        "\n",
        "\n",
        "def load_train_test_data(test_ratio=.3, random_state = 1):\n",
        "    df = pd.read_csv('./car.data', names=['buying', 'maint',\n",
        "                     'doors', 'persons', 'lug_boot', 'safety', 'target'])\n",
        "    X = df.drop(columns=['target'])\n",
        "    X = np.array(X.values)\n",
        "    y = np.array(df['target'].values)\n",
        "    label = np.unique(y)\n",
        "    # label encoding\n",
        "    for i in range(len(y)):\n",
        "        for j in range(len(label)):\n",
        "            if y[i] == label[j]:\n",
        "                y[i] = j\n",
        "                break\n",
        "    y = y.astype('int')\n",
        "\n",
        "    # data encoding\n",
        "    for i in range(X.shape[1]):\n",
        "        data = np.unique(X[:,i])\n",
        "        for j in range(X.shape[0]):\n",
        "            for n in range(len(data)):\n",
        "                if X[j,i] == data[n]:\n",
        "                    X[j,i] = n\n",
        "                    break\n",
        "    X = X.astype('int')\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size = test_ratio, random_state=random_state, stratify=y)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def accuracy_report(X_train_scale, y_train,X_test_scale,y_test,max_depth=7):\n",
        "    tree = DecisionTreeClassifier( max_depth=max_depth)\n",
        "    tree.fit(X_train_scale, y_train)\n",
        "    pred = tree.predict(X_train_scale)\n",
        "\n",
        "    print(\" tree train accuracy: %f\"\n",
        "        % (sklearn.metrics.accuracy_score(y_train, pred )))\n",
        "    pred = tree.predict(X_test_scale)\n",
        "    print(\" tree test accuracy: %f\"\n",
        "        % (sklearn.metrics.accuracy_score(y_test, pred )))\n",
        "\n",
        "    for i in range(10):\n",
        "        print(\"=============Cut=============\")\n",
        "        tree._prune()\n",
        "        pred = tree.predict(X_train_scale)\n",
        "        print(\" tree train accuracy: %f\"\n",
        "              % (sklearn.metrics.accuracy_score(y_train, pred)))\n",
        "        pred = tree.predict(X_test_scale)\n",
        "        print(\" tree test accuracy: %f\"\n",
        "              % (sklearn.metrics.accuracy_score(y_test, pred)))\n",
        "\n",
        "\n",
        "def main():\n",
        "    X_train, X_test, y_train, y_test = load_train_test_data(test_ratio=.3,random_state = 1)\n",
        "    accuracy_report(X_train, y_train,X_test,y_test,max_depth=8)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}